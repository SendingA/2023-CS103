<<<<<<< HEAD
| Input | Input |        |      |      | Output |
| :---: | :---: | :----: | :--: | :--: | :----: |
|   A   |   B   | $\sum$ |  f   |  G   |   C    |
|   0   |   0   |   0    |  0   |  0   |   0    |
|   0   |   1   |  0.5   |  0   |  0   |   0    |
|   1   |   0   |  0.5   |  0   |  0   |   0    |
|   1   |   1   |   1    |  1   |  1   |   1    |







| Number | Real Label | Predicted Result |
| :----: | :--------: | :--------------: |
|   1    |  (1,0,0)   |  (0.6,0.3,0.1)   |
|   2    |  (0,1,0)   |  (0.2,0.5,0.3)   |
|   3    |  (0,0,1)   | (0.0,0.25,0.75)  |

## 12-1

Here, we first calculate the cross entropy of each sample using the following formula: 

$CrossEntropy(y^{1},\hat{y}^{1})=-log0.6\approx0.5108$

$CrossEntropy(y^{2},\hat{y}^{2})=-log0.5\approx0.6931$

$CrossEntropy(y^{3},\hat{y}^{3})=-log0.75\approx0.2877$

According to the definition of the cross entropy loss function, calculate the cross entropy loss for all sample categories as follows ï¼š

 $L=\frac{1}{3} \sum_{i=1}^{3}CrossEntropy(y^{i},\hat{y}^{i})\approx0.4972 $

## 12.2

$Dice(X,Y)=\frac{2\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2+\sum_{i=1}^{n}y_i^2}$

â€‹					$=\frac{2*(0.83+0.85+0.77+0.91+0.94+0.82+0.97)}{7+5.4697}=\frac{12.18}{12.4697}\approx0.9768$



## 12.3

$MSE=\frac{1}{4}\sum_{i=1}^{4}(\hat{y_i}-y_i)^2=1.51/4=0.3775$











=======
| Input | Input |        |      |      | Output |
| :---: | :---: | :----: | :--: | :--: | :----: |
|   A   |   B   | $\sum$ |  f   |  G   |   C    |
|   0   |   0   |   0    |  0   |  0   |   0    |
|   0   |   1   |  0.5   |  0   |  0   |   0    |
|   1   |   0   |  0.5   |  0   |  0   |   0    |
|   1   |   1   |   1    |  1   |  1   |   1    |







| Number | Real Label | Predicted Result |
| :----: | :--------: | :--------------: |
|   1    |  (1,0,0)   |  (0.6,0.3,0.1)   |
|   2    |  (0,1,0)   |  (0.2,0.5,0.3)   |
|   3    |  (0,0,1)   | (0.0,0.25,0.75)  |

## 12-1

Here, we first calculate the cross entropy of each sample using the following formula: 

$CrossEntropy(y^{1},\hat{y}^{1})=-log0.6\approx0.5108$

$CrossEntropy(y^{2},\hat{y}^{2})=-log0.5\approx0.6931$

$CrossEntropy(y^{3},\hat{y}^{3})=-log0.75\approx0.2877$

According to the definition of the cross entropy loss function, calculate the cross entropy loss for all sample categories as follows ï¼š

 $L=\frac{1}{3} \sum_{i=1}^{3}CrossEntropy(y^{i},\hat{y}^{i})\approx0.4972 $

## 12.2

$Dice(X,Y)=\frac{2\sum_{i=1}^{n}x_iy_i}{\sum_{i=1}^{n}x_i^2+\sum_{i=1}^{n}y_i^2}$

â€‹					$=\frac{2*(0.83+0.85+0.77+0.91+0.94+0.82+0.97)}{7+5.4697}=\frac{12.18}{12.4697}\approx0.9768$



## 12.3

$MSE=\frac{1}{4}\sum_{i=1}^{4}(\hat{y_i}-y_i)^2=1.51/4=0.3775$











>>>>>>> 8fa95e816edbdda8676eab89fa81037c8f0f407a
